
<h2>Multi Armed Bandits Papers</h2>


<ul>

                             

 <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(1).pdf" style="text-decoration:none;">Thompson Sampling for Contextual Bandits with Linear Payoffs</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(2).pdf" style="text-decoration:none;">Taming the Monster:
A Fast and Simple Algorithm for Contextual Bandits</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(3).pdf" style="text-decoration:none;">A Survey on Contextual Multi-armed Bandits</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(4).pdf" style="text-decoration:none;">A Survey of Online Experiment Design with the Stochastic Multi-Armed Bandit</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(5).pdf" style="text-decoration:none;">Variational inference for the multi-armed contextual bandit</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(6).pdf" style="text-decoration:none;">Medoids in almost linear time via multi-armed bandits</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(7).pdf" style="text-decoration:none;">Learning Structural Weight Uncertainty for Sequential Decision-Making</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(8).pdf" style="text-decoration:none;"> Contextual Bandits with Stochastic Experts </a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(9).pdf" style="text-decoration:none;">Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling</a></li>
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(10).pdf" style="text-decoration:none;">Semiparametric Contextual Bandits </a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(11).pdf" style="text-decoration:none;">Learning Contextual Bandits in a Non-stationary Environment</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(12).pdf" style="text-decoration:none;">Myopic Bayesian Design of Experiments via Posterior Sampling and Probabilistic Programming</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(13).pdf" style="text-decoration:none;">Greybox fuzzing as a contextual bandits problem</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(14).pdf" style="text-decoration:none;">On-line Adaptative Curriculum Learning for GANs</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(15).pdf" style="text-decoration:none;">Machine Teaching of Active Sequential Learners</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(16).pdf" style="text-decoration:none;">Decentralized Cooperative Stochastic Bandits</a></li>

  <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(17).pdf" style="text-decoration:none;">Deep Reinforcement Learning based Recommendation with Explicit User-Item Interactions Modeling</a></li>   
  
<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(18).pdf" style="text-decoration:none;">Stay With Me: Lifetime Maximization Through Heteroscedastic Linear Bandits With Reneging</a></li> 

  
<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(19).pdf" style="text-decoration:none;">Practical Bayesian Neural Networks via Adaptive Optimization Methods</a></li> 

<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(20).pdf" style="text-decoration:none;">Adapting multi-armed bandits policies to contextual bandits scenarios</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(21).pdf" style="text-decoration:none;">Warm-starting Contextual Bandits:
Robustly Combining Supervised and Bandit Feedback</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(22).pdf" style="text-decoration:none;">The Assistive Multi-Armed Bandit</a></li> 
 <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(23).pdf" style="text-decoration:none;">From Complexity to Simplicity: Adaptive ES-Active Subspaces for Blackbox Optimization</a></li> 
 

   <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(24).pdf" style="text-decoration:none;">Batched Multi-armed Bandits Problem</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(25).pdf" style="text-decoration:none;">Introduction to Multi-Armed Bandits</a></li>                              
 <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(26).pdf" style="text-decoration:none;">Regret Bounds for Thompson Sampling in Episodic Restless Bandit Problems</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(27).pdf" style="text-decoration:none;">Model selection for contextual bandits</a></li>
   
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(28).pdf" style="text-decoration:none;">Distribution oblivious, risk-aware algorithms for multi-armed bandits with unbounded rewards</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(29).pdf" style="text-decoration:none;">Empirical Likelihood for Contextual Bandits</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(30).pdf" style="text-decoration:none;">Intrinsically Efficient, Stable, and Bounded Off-Policy Evaluation for Reinforcement Learning</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(31).pdf" style="text-decoration:none;">Bayesian Optimisation over Multiple Continuous and Categorical Inputs</a></li> 
    <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(32).pdf" style="text-decoration:none;">Censored Semi-Bandits: A Framework for Resource Allocation with Censored Feedback</a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(33).pdf" style="text-decoration:none;">Practical Calculation of Gittins Indices for Multi-armed Bandits</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(34).pdf" style="text-decoration:none;">Model-free Reinforcement Learning in Infinite-horizon Average-reward Markov Decision Processes</a></li> 
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(35).pdf" style="text-decoration:none;">Thompson Sampling via Local Uncertainty</a></li> 

  <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(36).pdf" style="text-decoration:none;">Persistency of Excitation for Robustness of Neural Networks</a></li> 
 
<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(37).pdf" style="text-decoration:none;">Neural Contextual Bandits with UCB-based Exploration</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(38).pdf" style="text-decoration:none;">Safe Exploration for Optimizing Contextual Bandits</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(39).pdf" style="text-decoration:none;">Adaptive Estimator Selection for Off-Policy Evaluation</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(40).pdf" style="text-decoration:none;">Hierarchical Adaptive Contextual Bandits for Resource Constraint based Recommendation</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(41).pdf" style="text-decoration:none;">Thompson Sampling for Linearly Constrained Bandits</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(42).pdf" style="text-decoration:none;">An Empirical Study of Human Behavioral Agents in Bandits, Contextual Bandits and Reinforcement Learning</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(43).pdf" style="text-decoration:none;">Gaussian Gated Linear Networks</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(44).pdf" style="text-decoration:none;">Online Learning in Iterated Prisoner's Dilemma to Mimic Human Behavior</a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(45).pdf" style="text-decoration:none;">Finding All &epsilon;-Good Arms in Stochastic Bandits</a></li>  
   
<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(46).pdf" style="text-decoration:none;">Recurrent Neural-Linear Posterior Sampling for Non-Stationary Contextual Bandits</a></li> 
                             
<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(47).pdf" style="text-decoration:none;">Lenient Regret for Multi-Armed Bandits</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(48).pdf" style="text-decoration:none;">Using Subjective Logic to Estimate Uncertainty in Multi-Armed Bandit Problems</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(49).pdf" style="text-decoration:none;">Carousel Personalization in Music Streaming Apps with Contextual Bandits</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(50).pdf" style="text-decoration:none;">Dual-Mandate Patrols: Multi-Armed Bandits for Green Security</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(51).pdf" style="text-decoration:none;">Online Semi-Supervised Learning in Contextual Bandits with Episodic Reward</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(52).pdf" style="text-decoration:none;">Offline Contextual Bandits with
High Probability Fairness Guarantees</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(53).pdf" style="text-decoration:none;">Thompson Sampling for Multinomial Logit Contextual Bandits</a></li>
 
<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(54).pdf" style="text-decoration:none;">Residual Loss Prediction: Reinforcement Learning with no Incremental Feedback </a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multi-Armed-Bandits-Papers/blob/master/mub(55).pdf" style="text-decoration:none;">SIC -MMAB: Synchronisation Involves Communication in Multiplayer Multi-Armed Bandits</a></li>
 
  </ul>
  
  
